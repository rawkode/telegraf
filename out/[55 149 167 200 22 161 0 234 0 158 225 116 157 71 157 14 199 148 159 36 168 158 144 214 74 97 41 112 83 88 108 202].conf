[agent]
  interval = "1s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "1s"
  flush_jitter = "0s"
  precision = ""
  debug = false
  quiet = false
  logfile = ""
  hostname = ""
  omit_hostname = false

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

[[outputs.remp_elasticsearch]]
  urls = ["http://elasticsearch:9200"] # required
  timeout = "5s"
  index_name = "pageviews"
  type_name = "_doc"
  namepass = ["pageviews"]
  manage_template = false
  id_field = "remp_pageview_id"
  tagexclude = ["host"]

[[outputs.remp_elasticsearch]]
  urls = ["http://elasticsearch:9200"] # required
  timeout = "5s"
  index_name = "pageviews_time_spent"
  type_name = "_doc"
  namepass = ["pageviews_time_spent"]
  manage_template = false
  id_field = "remp_pageview_id"
  updated_fields = ["timespent"]
  tagexclude = ["host"]
  field_whitelist = ["token", "subscriber", "signed_in", "locked", "timespent", "unload"]

[[outputs.remp_elasticsearch]]
  urls = ["http://elasticsearch:9200"] # required
  timeout = "5s"
  index_name = "pageviews_progress"
  type_name = "_doc"
  namepass = ["pageviews_progress"]
  manage_template = false
  id_field = "remp_pageview_id"
  updated_fields = ["page_progress", "article_progress"]
  field_whitelist = ["token", "subscriber", "signed_in", "locked", "page_progress", "article_progress", "unload"]

[[outputs.remp_elasticsearch]]
  urls = ["http://elasticsearch:9200"] # required
  timeout = "5s"
  index_name = "events"
  type_name = "_doc"
  namepass = ["events_v2"]
  manage_template = false
  id_field = "remp_event_id"
  tagexclude = ["host"]

[[outputs.remp_elasticsearch]]
  urls = ["http://elasticsearch:9200"] # required
  timeout = "5s"
  index_name = "commerce"
  type_name = "_doc"
  namepass = ["commerce"]
  manage_template = false
  id_field = "remp_commerce_id"
  tagexclude = ["host"]

[[outputs.remp_elasticsearch]]
  urls = ["http://elasticsearch:9200"] # required
  timeout = "5s"
  index_name = "concurrents_by_browser"
  type_name = "_doc"
  # if you to track timespent, keep current setting as is so the concurrents can be based on more recent data
  namepass = ["pageviews_time_spent"]
  # if you don't plan to track timespent, you can make concurrents_by_browser based on regular pageviews; the data will be a bit less accurate
  # namepass = ["pageviews"]
  manage_template = false
  id_field = "browser_id"
  # TODO replace URL with canonical URL, without URL fragments
  updated_fields = ["time", "article_id", "token", "derived_referer_host_with_path", "derived_referer_medium", "explicit_referer_medium", "url"]
  field_whitelist = ["time", "article_id", "token", "derived_referer_host_with_path", "derived_referer_medium", "explicit_referer_medium", "url"]

[[outputs.remp_elasticsearch]]
  urls = ["http://elasticsearch:9200"] # required
  timeout = "5s"
  index_name = "entities"
  type_name = "_doc"
  namepass = ["entities"]
  manage_template = false
  id_field = "remp_entity_id"
  tagexclude = ["host"]

###############################################################################
#                            SERVICE INPUT PLUGINS                            #
###############################################################################

[[inputs.kafka_consumer]]
  topics = ["beam_events"]
  brokers = ["kafka:9092"]
  consumer_group = "beam_consumers"
  offset = "oldest"
  data_format = "influx"
  max_undelivered_messages = 10
